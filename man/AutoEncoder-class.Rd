% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoencoder.R
\docType{class}
\name{AutoEncoder-class}
\alias{AutoEncoder-class}
\alias{AutoEncoder}
\title{AutoEncoder}
\description{
An S4 Class implementing an Autoencoder
}
\details{
Autoencoders are neural networks that try to reproduce their input.
}
\section{Slots}{

\describe{
\item{\code{fun}}{A function that does the embedding and returns a
dimRedResult object.}

\item{\code{stdpars}}{The standard parameters for the function.}
}}

\section{General usage}{

Dimensionality reduction methods are S4 Classes that either be used
directly, in which case they have to be initialized and a full
list with parameters has to be handed to the \code{@fun()}
slot, or the method name be passed to the embed function and
parameters can be given to the \code{...}, in which case
missing parameters will be replaced by the ones in the
\code{@stdpars}.
}

\section{Parameters}{

Autoencoder can take the following parameters:
\describe{
  \item{ndim}{The number of dimensions for reduction.}
  \item{n_hidden}{The number of neurons in the hidden
      layers, the length specifies the number of layers,
      the length must be impair, the middle number must
      be the same as ndim.}
  \item{activation}{The activation functions for the layers,
      one of "tanh", "sigmoid", "relu", "elu", everything
      else will silently be ignored and there will be no
      activation function for the layer.}
  \item{weight_decay}{the coefficient for weight decay,
      set to 0 if no weight decay desired.}
  \item{learning_rate}{The learning rate for gradient descend}
  \item{graph}{Optional: A list of bits and pieces that define the
     autoencoder in tensorflow, see details.}
  \item{keras_graph}{Optional: A list of keras layers that define
     the encoder and decoder, specifying this, will ignore all
     other topology related variables, see details.}
  \item{batchsize}{If NA, all data will be used for training,
      else only a random subset of size batchsize will be used}
  \item{n_steps}{the number of training steps.}
}
}

\section{Details}{


There are several ways to specify an autoencoder, the simplest is to pass the
number of neurons per layer in \code{n_hidden}, this must be a vector of
integers of impair length and it must be symmetric and the middle number must
be equal to \code{ndim}, For every layer an activation function can be specified
with \code{activation}.

For regularization weight decay can be specified by setting
\code{weight_decay} > 0.

Currently only a gradient descent optimizer is used, the learning rate can be
specified by setting \code{learning_rate}.
The learner can operate on batches if \code{batchsize} is not \code{NA}.
The number of steps the learner uses is specified using \code{n_steps}.
}

\examples{
\dontrun{
dat <- loadDataSet("3D S Curve")

## use the S4 Class directly:
autoenc <- AutoEncoder()
emb <- autoenc@fun(dat, autoenc@stdpars)

## simpler, use embed():
emb2 <- embed(dat, "AutoEncoder")

plot(emb, type = "2vars")

samp <- sample(floor(nrow(dat) / 10))
embsamp <- autoenc@fun(dat[samp], autoenc@stdpars)
embother <- embsamp@apply(dat[-samp])
plot(embsamp, type = "2vars")
points(embother@data)
}

}
\seealso{
Other dimensionality reduction methods: \code{\link{DRR-class}},
  \code{\link{DiffusionMaps-class}},
  \code{\link{DrL-class}}, \code{\link{FastICA-class}},
  \code{\link{FruchtermanReingold-class}},
  \code{\link{HLLE-class}}, \code{\link{Isomap-class}},
  \code{\link{KamadaKawai-class}}, \code{\link{LLE-class}},
  \code{\link{MDS-class}}, \code{\link{PCA-class}},
  \code{\link{PCA_L1-class}},
  \code{\link{dimRedMethod-class}},
  \code{\link{kPCA-class}}, \code{\link{nMDS-class}},
  \code{\link{tSNE-class}}
}
